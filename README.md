# Self-Teaching Transformers

This repository is a self-study project to understand transformers by implementing them from scratch.  
The work follows the schema from the well known paper ["Attention Is All You Need"](https://arxiv.org/pdf/1706.03762), starting from a simple bigram baseline and gradually building up to a transformer with self-attention and multi-head attention.  
The aim of the project was to stitch together a LLM based on writing (which you will not find in this repo as that would be illegal) from one of my favorite authors Terry Pratchett - who has sadly deceased. But, as I should've learnt from his books, some things aren't quite the same after you've stiched them back together and should perhaps be left... unalive.  

![Transformer Schema](https://i.imgur.com/jw9V789.png)

---

## Whatâ€™s Implemented

- **Naive Bigram Model (`train.py`, `bigram_simple.py`)**  
  A minimal character-level model where each token directly predicts the next one from an embedding lookup.  
  This serves as the baseline for later improvements.

- **Self-Attention & Multi-Head Attention (`bigram_self_att.py`)**  
  Extends the bigram baseline into a small transformer model:  
  - Implements self-attention and expands it to multi-head attention.  
  - Adds transformer blocks with feed-forward layers, residual connections, and layer normalization.  
  - Includes a training loop and generation function to produce new text.

- **Generated Output (`output.txt`)**  
  Example text generated by the self-attention model. While far from coherent, the samples show emerging structure, punctuation, and rhythm.

---

## Project Goals

- Build an understanding of embeddings and token prediction through bigram models.  
- Introduce self-attention to capture dependencies beyond immediate neighbors.  
- Expand to multi-head attention and transformer blocks to observe how structure emerges.  
- Document the progression from simple baselines to more complex architectures as a learning process.  
- Channel my inner Igor to create my personal Terry Pratchett creature. Hopefully with less lightning.
